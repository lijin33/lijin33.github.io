---
layout:     post              # 使用的布局（不需要改）
title:      Spark job运行出错记录       # 标题 
subtitle:                  #副标题
date:       2018-05-03        # 时间
author:     Lacia           # 作者
header-img: img/post-bg-2015.jpg  #这篇文章标题背景图片
catalog: true             # 是否归档
tags:               #标签
    - Spark
    - Yarn
---

#### Spark Job运行出错记录

运行spark job的时候出现以下问题：

```
18/04/26 14:31:43 INFO yarn.Client: Setting up container launch context for our AM
18/04/26 14:31:43 INFO yarn.Client: Setting up the launch environment for our AM container
18/04/26 14:31:43 INFO yarn.Client: Preparing resources for our AM container
18/04/26 14:31:44 INFO yarn.Client: Uploading resource file:/tmp/spark-bfdaa58d-88a2-4e36-91cb-3b96f0440d68/__spark_conf__8076600052395851378.zip -> hdfs://n1:8020/user/root/.sparkStaging/application_1524710713169_0005/__spark_conf__8076600052395851378.zip
18/04/26 14:31:40 INFO yarn.Client: Application report for application_1524710713169_0004 (state: ACCEPTED)
18/04/26 14:31:41 INFO yarn.Client: Application report for application_1524710713169_0004 (state: ACCEPTED)
18/04/26 14:31:42 INFO yarn.Client: Application report for application_1524710713169_0004 (state: ACCEPTED)
18/04/26 14:31:43 INFO yarn.Client: Application report for application_1524710713169_0004 (state: ACCEPTED)
18/04/26 14:31:44 INFO yarn.Client: Application report for application_1524710713169_0004 (state: ACCEPTED)
```

两个job，只能运行一个，一开始以为是内存问题，将内存和yarn的可用内存调大之后依旧不行。

后来查看yarn resource manager时发现，cdh所在的主机vcore只有两个，不足以支持两个job同时运行…

查看了一下虚拟机的配置信息，发现该虚拟机CPU为单核_(:з」∠)_，改为8核后重启就好了



![mark](http://owl3le8ji.bkt.clouddn.com/blog/180427/jBGDHeag91.png?imageslim)



*P.S：*记录三个命令

```
#查看正在运行的application
yarn application -list
#kill掉指定application
yarn application -kill appid
#查看正在运行的spark job
jps -ml | grep spark
```

